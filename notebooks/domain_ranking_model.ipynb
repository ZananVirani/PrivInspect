{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a4adc7",
   "metadata": {},
   "source": [
    "# PrivInspect ML Model Training - Complete Setup Guide\n",
    "\n",
    "This notebook provides a complete end-to-end setup for training and deploying the PrivInspect domain ranking model on any fresh device.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.8+ installed\n",
    "- Internet connection (for downloading dependencies and TrackerRadar data)\n",
    "- ~2GB free disk space for data and model files\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Installs all dependencies** automatically\n",
    "2. **Downloads DuckDuckGo TrackerRadar data** (51,198 domains)\n",
    "3. **Trains ML model** with aggressive privacy detection\n",
    "4. **Tests model performance** on known tracking/legitimate domains\n",
    "5. **Verifies integration** with FastAPI backend\n",
    "6. **Saves trained models** for production use\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- Fresh install: ~10-15 minutes\n",
    "- Model training: ~5-10 minutes\n",
    "- Total: ~20-25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942d676",
   "metadata": {},
   "source": [
    "# Step 1: Install Dependencies\n",
    "\n",
    "Automatically installs all required Python packages for machine learning, data processing, and visualization. This ensures the notebook works on any fresh device without manual setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5577e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install all required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def install_package(package_name, import_name=None):\n",
    "    \"\"\"Install a package and verify it can be imported\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"‚úÖ {package_name} already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"‚è≥ Installing {package_name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"‚úÖ {package_name} installed successfully\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Core dependencies for ML and data processing\n",
    "packages = [\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"), \n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    (\"lightgbm\", \"lightgbm\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"seaborn\", \"seaborn\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"joblib\", \"joblib\"),\n",
    "    (\"tqdm\", \"tqdm\")\n",
    "]\n",
    "\n",
    "print(\"üîß Installing required packages...\")\n",
    "failed_packages = []\n",
    "\n",
    "for package_name, import_name in packages:\n",
    "    success = install_package(package_name, import_name)\n",
    "    if not success:\n",
    "        failed_packages.append(package_name)\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"‚ùå Failed to install: {failed_packages}\")\n",
    "    print(\"Please install these manually and rerun the notebook\")\n",
    "else:\n",
    "    print(\"‚úÖ All packages installed successfully!\")\n",
    "    \n",
    "# Verify Python version\n",
    "print(f\"\\nüêç Python version: {sys.version}\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Try to import all packages to verify installation\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import sklearn\n",
    "    import lightgbm as lgb\n",
    "    import requests\n",
    "    import joblib\n",
    "    from tqdm import tqdm\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Please restart the notebook kernel and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724cc66",
   "metadata": {},
   "source": [
    "# Step 2: Project Setup and Data Download\n",
    "\n",
    "Creates the necessary project directory structure and defines the TrackerRadar data download functionality. This sets up everything needed to fetch and organize the DuckDuckGo TrackerRadar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd234434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Setup project structure and create training module\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create necessary directories\n",
    "directories = ['../data', '../models', '../scripts']\n",
    "for dir_path in directories:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"üìÅ Created/verified directory: {dir_path}\")\n",
    "\n",
    "# Define the complete training module inline (for standalone operation)\n",
    "class TrackerRadarParser:\n",
    "    \"\"\"Parser for DuckDuckGo TrackerRadar data\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=\"https://github.com/duckduckgo/tracker-radar/archive/refs/heads/main.zip\"):\n",
    "        self.base_url = base_url\n",
    "        self.data_dir = Path(\"../data\")\n",
    "        \n",
    "    def download_tracker_radar(self) -> Dict:\n",
    "        \"\"\"Download and parse TrackerRadar data\"\"\"\n",
    "        import zipfile\n",
    "        import tempfile\n",
    "        \n",
    "        print(\"üì• Downloading DuckDuckGo TrackerRadar data...\")\n",
    "        \n",
    "        # Download the zip file\n",
    "        response = requests.get(self.base_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Extract to temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            zip_path = Path(temp_dir) / \"tracker-radar.zip\"\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            # Extract zip\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_dir)\n",
    "            \n",
    "            # Find the extracted folder\n",
    "            extracted_folders = [d for d in Path(temp_dir).iterdir() if d.is_dir() and 'tracker-radar' in d.name]\n",
    "            if not extracted_folders:\n",
    "                raise ValueError(\"Could not find tracker-radar folder in downloaded zip\")\n",
    "            \n",
    "            tracker_radar_path = extracted_folders[0]\n",
    "            \n",
    "            # Copy to our data directory\n",
    "            import shutil\n",
    "            target_path = self.data_dir / \"tracker-radar\"\n",
    "            if target_path.exists():\n",
    "                shutil.rmtree(target_path)\n",
    "            shutil.copytree(tracker_radar_path, target_path)\n",
    "            \n",
    "            print(f\"‚úÖ TrackerRadar data downloaded to {target_path}\")\n",
    "            \n",
    "            # Return basic info about the data\n",
    "            domains_path = target_path / \"domains\"\n",
    "            if domains_path.exists():\n",
    "                # Count domain files across all countries\n",
    "                domain_count = 0\n",
    "                country_dirs = [d for d in domains_path.iterdir() if d.is_dir()]\n",
    "                for country_dir in country_dirs:\n",
    "                    json_files = list(country_dir.glob(\"*.json\"))\n",
    "                    domain_count += len(json_files)\n",
    "                \n",
    "                return {\n",
    "                    \"domains\": domain_count,\n",
    "                    \"countries\": len(country_dirs),\n",
    "                    \"data_path\": str(target_path)\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(\"Downloaded data does not contain expected domains directory\")\n",
    "\n",
    "print(\"‚úÖ TrackerRadarParser class defined\")\n",
    "print(\"üöÄ Ready to download TrackerRadar data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63c74a",
   "metadata": {},
   "source": [
    "# Step 3: Execute Data Download\n",
    "\n",
    "Downloads the complete DuckDuckGo TrackerRadar dataset from GitHub, extracts it, and verifies the download. This provides the raw data for training the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da4d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download TrackerRadar data\n",
    "parser = TrackerRadarParser()\n",
    "\n",
    "try:\n",
    "    # Download the data\n",
    "    tracker_info = parser.download_tracker_radar()\n",
    "    \n",
    "    print(\"üìä TrackerRadar Data Summary:\")\n",
    "    print(f\"   Total domains: {tracker_info['domains']:,}\")\n",
    "    print(f\"   Countries: {tracker_info['countries']}\")\n",
    "    print(f\"   Data location: {tracker_info['data_path']}\")\n",
    "    \n",
    "    # Verify the download\n",
    "    data_path = Path(tracker_info['data_path'])\n",
    "    if data_path.exists():\n",
    "        print(\"‚úÖ TrackerRadar data successfully downloaded and verified!\")\n",
    "    else:\n",
    "        print(\"‚ùå Data verification failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to download TrackerRadar data: {e}\")\n",
    "    print(\"Please check your internet connection and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880500db",
   "metadata": {},
   "source": [
    "# Step 4: Define Advanced Feature Extraction\n",
    "\n",
    "Creates the domain feature extraction system with aggressive privacy detection improvements. This includes enhanced category-based tracking detection, legitimate domain protection, and reduced importance of resource counts for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12156d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define domain feature extraction with aggressive improvements\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class DomainFeatureExtractor:\n",
    "    \"\"\"Extract features from TrackerRadar domain data with aggressive privacy detection\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.domains_path = self.data_path / \"domains\"\n",
    "        \n",
    "    def parse_domain_json(self, json_path: Path) -> Optional[Dict]:\n",
    "        \"\"\"Parse a single domain JSON file with aggressive category-based scoring\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            domain_name = json_path.stem\n",
    "            \n",
    "            # Get base fingerprinting score\n",
    "            base_fingerprinting = data.get('fingerprinting', 0)\n",
    "            \n",
    "            # AGGRESSIVE category-based tracking detection\n",
    "            categories = data.get('categories', [])\n",
    "            tracking_categories = [\n",
    "                'Ad Motivated Tracking', 'Advertising', 'Analytics', \n",
    "                'Audience Measurement', 'Third-Party Analytics Marketing',\n",
    "                'Cross-site Tracking', 'Fingerprinting'\n",
    "            ]\n",
    "            \n",
    "            # Count tracking categories with very aggressive weighting\n",
    "            tracking_category_count = sum(1 for cat in categories if cat in tracking_categories)\n",
    "            \n",
    "            # Additional aggressive categories\n",
    "            aggressive_categories = ['Social - Share', 'Embedded Content']\n",
    "            aggressive_count = sum(1 for cat in categories if cat in aggressive_categories)\n",
    "            \n",
    "            # VERY AGGRESSIVE scoring for multi-category tracking domains\n",
    "            if tracking_category_count >= 4:  # Domains with 4+ tracking categories\n",
    "                category_tracking_score = min(3, 2.5 + (tracking_category_count * 0.3) + (aggressive_count * 0.5))\n",
    "            elif tracking_category_count >= 2:  # Domains with 2-3 tracking categories  \n",
    "                category_tracking_score = min(3, 2.0 + (tracking_category_count * 0.4) + (aggressive_count * 0.4))\n",
    "            else:  # Single or no tracking categories\n",
    "                category_tracking_score = min(3, (tracking_category_count * 1.0) + (aggressive_count * 1.5))\n",
    "            \n",
    "            # Use maximum of base fingerprinting and enhanced category score\n",
    "            enhanced_fingerprinting = max(base_fingerprinting, category_tracking_score)\n",
    "            \n",
    "            # Reduce num_resources importance by 70% (aggressive)\n",
    "            raw_num_resources = len(data.get('resources', []))\n",
    "            scaled_num_resources = raw_num_resources * 0.3  # 70% reduction\n",
    "            \n",
    "            # Legitimate domain allowlisting\n",
    "            legitimate_domains = {\n",
    "                'wikipedia.org', 'archive.org', 'mozilla.org', 'github.com',\n",
    "                'stackoverflow.com', 'reddit.com', 'medium.com', 'twitter.com'\n",
    "            }\n",
    "            \n",
    "            # Reduce global_prevalence impact for legitimate sites\n",
    "            global_prevalence = data.get('prevalence', 0.0)\n",
    "            if domain_name in legitimate_domains:\n",
    "                global_prevalence = min(global_prevalence, 0.0005)  # Aggressive capping\n",
    "            \n",
    "            # Extract comprehensive features\n",
    "            resources = data.get('resources', [])\n",
    "            resource_types = {}\n",
    "            resource_fingerprinting_scores = []\n",
    "            \n",
    "            for resource in resources:\n",
    "                res_type = resource.get('type', 'unknown')\n",
    "                resource_types[res_type] = resource_types.get(res_type, 0) + 1\n",
    "                \n",
    "                res_fingerprinting = resource.get('fingerprinting', 0)\n",
    "                if res_fingerprinting > 0:\n",
    "                    resource_fingerprinting_scores.append(res_fingerprinting)\n",
    "            \n",
    "            features = {\n",
    "                'domain': domain_name,\n",
    "                'fingerprinting': enhanced_fingerprinting,\n",
    "                'cookies_prevalence': data.get('cookies', 0.0),\n",
    "                'global_prevalence': global_prevalence,\n",
    "                'num_sites': data.get('sites', 0),\n",
    "                'num_subdomains': len(data.get('subdomains', [])),\n",
    "                'num_cnames': len(data.get('cnames', [])),\n",
    "                'num_resources': scaled_num_resources,\n",
    "                'num_top_initiators': len(data.get('topInitiators', [])),\n",
    "                'owner_present': 1 if data.get('owner') else 0,\n",
    "                'resource_type_script_count': resource_types.get('script', 0),\n",
    "                'resource_type_xhr_count': resource_types.get('xmlhttprequest', 0),\n",
    "                'resource_type_image_count': resource_types.get('image', 0),\n",
    "                'resource_type_css_count': resource_types.get('stylesheet', 0),\n",
    "                'resource_type_font_count': resource_types.get('font', 0),\n",
    "                'resource_type_media_count': resource_types.get('media', 0),\n",
    "                'avg_resource_fingerprinting': np.mean(resource_fingerprinting_scores) if resource_fingerprinting_scores else 0.0,\n",
    "                'has_example_sites': 1 if data.get('exampleSites') else 0\n",
    "            }\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {json_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_all_features(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Extract features from all domain files\"\"\"\n",
    "        all_features = {}\n",
    "        \n",
    "        if not self.domains_path.exists():\n",
    "            raise ValueError(f\"Domains directory not found: {self.domains_path}\")\n",
    "        \n",
    "        # Process all country directories\n",
    "        country_dirs = [d for d in self.domains_path.iterdir() if d.is_dir()]\n",
    "        total_processed = 0\n",
    "        \n",
    "        print(f\"üîç Processing domains from {len(country_dirs)} countries...\")\n",
    "        \n",
    "        for country_dir in country_dirs:\n",
    "            print(f\"üìç Processing {country_dir.name}...\")\n",
    "            json_files = list(country_dir.glob(\"*.json\"))\n",
    "            \n",
    "            for i, json_file in enumerate(json_files):\n",
    "                features = self.parse_domain_json(json_file)\n",
    "                if features:\n",
    "                    all_features[features['domain']] = features\n",
    "                    total_processed += 1\n",
    "                \n",
    "                # Progress update every 1000 files\n",
    "                if (i + 1) % 1000 == 0:\n",
    "                    print(f\"   Processed {i + 1:,}/{len(json_files):,} files\")\n",
    "        \n",
    "        print(f\"‚úÖ Extracted features for {total_processed:,} domains\")\n",
    "        return all_features\n",
    "\n",
    "print(\"‚úÖ DomainFeatureExtractor class defined with aggressive improvements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4fde08",
   "metadata": {},
   "source": [
    "# Step 5: Process All Domain Data\n",
    "\n",
    "Extracts features from all ~51,000+ domains in the TrackerRadar dataset. This processes each domain's JSON file to create the training dataset with comprehensive privacy-focused features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Extract features from all domains\n",
    "extractor = DomainFeatureExtractor(tracker_info['data_path'])\n",
    "\n",
    "print(\"üöÄ Starting feature extraction from all TrackerRadar domains...\")\n",
    "print(\"This may take 5-10 minutes for ~51,000 domains...\")\n",
    "\n",
    "# Extract features\n",
    "all_domain_features = extractor.extract_all_features()\n",
    "\n",
    "print(f\"\\nüìä Feature Extraction Summary:\")\n",
    "print(f\"   Total domains processed: {len(all_domain_features):,}\")\n",
    "\n",
    "# Show sample features from a few domains\n",
    "sample_domains = list(all_domain_features.keys())[:3]\n",
    "print(f\"\\nüîç Sample domain features:\")\n",
    "for domain in sample_domains:\n",
    "    features = all_domain_features[domain]\n",
    "    print(f\"\\n{domain}:\")\n",
    "    for key, value in list(features.items())[:8]:  # Show first 8 features\n",
    "        if key != 'domain':\n",
    "            print(f\"   {key}: {value}\")\n",
    "\n",
    "# Save features to file for later use\n",
    "features_file = Path(\"../models/domain_features_notebook.json\")\n",
    "with open(features_file, 'w') as f:\n",
    "    json.dump(all_domain_features, f)\n",
    "\n",
    "print(f\"\\n‚úÖ Features saved to: {features_file}\")\n",
    "print(\"‚úÖ Feature extraction completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94089b35",
   "metadata": {},
   "source": [
    "# Step 6: Train Machine Learning Model\n",
    "\n",
    "Creates training targets based on domain characteristics and trains a LightGBM model to predict domain tracking intensity. Includes model evaluation, feature importance analysis, and saves the trained model for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create training targets and train ML model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Train the domain risk model with LightGBM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def create_training_targets(self, features_dict: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Create training targets based on domain characteristics\"\"\"\n",
    "        targets = {}\n",
    "        \n",
    "        print(\"üéØ Creating training targets...\")\n",
    "        \n",
    "        for domain, features in features_dict.items():\n",
    "            # Base tracking intensity from fingerprinting\n",
    "            base_intensity = min(features['fingerprinting'] / 3.0, 1.0)\n",
    "            \n",
    "            # Boost for high cookie prevalence\n",
    "            cookie_boost = min(features['cookies_prevalence'] * 0.3, 0.3)\n",
    "            \n",
    "            # Boost for high prevalence (indicates tracking)\n",
    "            prevalence_boost = min(features['global_prevalence'] * 100, 0.2)\n",
    "            \n",
    "            # Resource-based tracking signals\n",
    "            resource_boost = 0\n",
    "            if features['num_resources'] > 10:  # Already scaled down\n",
    "                resource_boost = 0.1\n",
    "            if features['avg_resource_fingerprinting'] > 1:\n",
    "                resource_boost += 0.1\n",
    "                \n",
    "            # Combine all factors\n",
    "            tracking_intensity = min(base_intensity + cookie_boost + prevalence_boost + resource_boost, 1.0)\n",
    "            \n",
    "            # Apply some noise for regularization\n",
    "            noise = np.random.normal(0, 0.05)\n",
    "            tracking_intensity = max(0.0, min(1.0, tracking_intensity + noise))\n",
    "            \n",
    "            targets[domain] = tracking_intensity\n",
    "        \n",
    "        print(f\"‚úÖ Created targets for {len(targets):,} domains\")\n",
    "        return targets\n",
    "    \n",
    "    def train_model(self, features_dict: Dict, targets_dict: Dict):\n",
    "        \"\"\"Train the LightGBM model\"\"\"\n",
    "        print(\"üèãÔ∏è Training LightGBM model...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        domains = list(features_dict.keys())\n",
    "        \n",
    "        # Create feature matrix\n",
    "        feature_names = [k for k in features_dict[domains[0]].keys() if k != 'domain']\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for domain in domains:\n",
    "            if domain in targets_dict:\n",
    "                features = features_dict[domain]\n",
    "                feature_row = [features[name] for name in feature_names]\n",
    "                X.append(feature_row)\n",
    "                y.append(targets_dict[domain])\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        print(f\"Training data shape: {X.shape}\")\n",
    "        print(f\"Target range: {y.min():.3f} to {y.max():.3f}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train LightGBM model\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            \n",
    "            self.model = lgb.LGBMRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )\n",
    "            \n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = self.model.predict(X_test_scaled)\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            correlation, _ = spearmanr(y_test, y_pred)\n",
    "            \n",
    "            print(f\"\\\\nüìä Model Performance:\")\n",
    "            print(f\"   MAE: {mae:.4f}\")\n",
    "            print(f\"   RMSE: {rmse:.4f}\")\n",
    "            print(f\"   Spearman correlation: {correlation:.4f}\")\n",
    "            \n",
    "            # Feature importance\n",
    "            importances = self.model.feature_importances_\n",
    "            feature_importance = list(zip(feature_names, importances))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\\\nüîù Top 10 Feature Importances:\")\n",
    "            for i, (feature, importance) in enumerate(feature_importance[:10]):\n",
    "                print(f\"   {i+1:2d}. {feature}: {importance:.0f}\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ùå LightGBM not available, using RandomForest as fallback...\")\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            \n",
    "            self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = self.model.predict(X_test_scaled)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            print(f\"\\\\nüìä Model Performance (RandomForest):\")\n",
    "            print(f\"   MAE: {mae:.4f}\")\n",
    "            print(f\"   RMSE: {rmse:.4f}\")\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    def save_model(self, save_dir: str):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        save_path = Path(save_dir)\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        model_artifacts = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names,\n",
    "            'model_type': 'lightgbm' if hasattr(self.model, 'feature_importances_') else 'random_forest'\n",
    "        }\n",
    "        \n",
    "        model_file = save_path / \"domain_risk_model_notebook.pkl\"\n",
    "        joblib.dump(model_artifacts, model_file)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved to: {model_file}\")\n",
    "        return str(model_file)\n",
    "\n",
    "# Initialize and train the model\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Create targets\n",
    "targets = trainer.create_training_targets(all_domain_features)\n",
    "\n",
    "# Train model\n",
    "success = trainer.train_model(all_domain_features, targets)\n",
    "\n",
    "if success:\n",
    "    # Save model\n",
    "    model_path = trainer.save_model(\"../models\")\n",
    "    print(\"‚úÖ Model training completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Model training failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
