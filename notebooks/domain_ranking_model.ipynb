{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a4adc7",
   "metadata": {},
   "source": [
    "# PrivInspect ML Model Training - Complete Setup Guide\n",
    "\n",
    "This notebook provides a complete end-to-end setup for training and deploying the PrivInspect domain ranking model on any fresh device.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Python 3.X.X installed\n",
    "- Internet connection (for downloading dependencies and TrackerRadar data)\n",
    "- ~2GB free disk space for data and model files\n",
    "\n",
    "## What This Notebook Does\n",
    "\n",
    "1. **Installs all dependencies** automatically\n",
    "2. **Downloads DuckDuckGo TrackerRadar data** (51,198 domains)\n",
    "3. **Trains ML model** with aggressive privacy detection\n",
    "4. **Tests model performance** on known tracking/legitimate domains\n",
    "5. **Verifies integration** with FastAPI backend\n",
    "6. **Saves trained models** for production use\n",
    "\n",
    "## Expected Runtime\n",
    "\n",
    "- Fresh install: ~10-15 minutes\n",
    "- Model training: ~5-10 minutes\n",
    "- Total: ~20-25 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1942d676",
   "metadata": {},
   "source": [
    "# Step 1: Install Dependencies\n",
    "\n",
    "**IMPORTANT: Virtual Environment Setup**\n",
    "\n",
    "Before running this notebook, please ensure you have created and activated a virtual environment as described in the main README:\n",
    "\n",
    "```bash\n",
    "# Create virtual environment (if not already done)\n",
    "python3 -m venv .venv\n",
    "\n",
    "# Activate virtual environment\n",
    "source .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "This notebook will automatically detect if you're running in a virtual environment and install packages accordingly. If no virtual environment is detected, it will warn you and proceed with system Python (not recommended).\n",
    "\n",
    "The cell below will automatically install all required Python packages for machine learning, data processing, and visualization to your **activated environment**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b5577e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking Python environment...\n",
      "‚úÖ Virtual environment detected: conda environment\n",
      "   Environment path: Unknown\n",
      "\n",
      "üîß Installing required packages to: /Users/zananvirani/Desktop/PrivInspect/backend/.venv\n",
      "‚úÖ pandas already installed\n",
      "‚úÖ numpy already installed\n",
      "‚úÖ scikit-learn already installed\n",
      "‚úÖ lightgbm already installed\n",
      "‚úÖ matplotlib already installed\n",
      "‚úÖ seaborn already installed\n",
      "‚úÖ requests already installed\n",
      "‚úÖ joblib already installed\n",
      "‚úÖ tqdm already installed\n",
      "‚úÖ All packages installed successfully!\n",
      "\n",
      "üêç Python Environment Info:\n",
      "   Python version: 3.13.1 (v3.13.1:06714517797, Dec  3 2024, 14:00:22) [Clang 15.0.0 (clang-1500.3.9.4)]\n",
      "   Python executable: /Users/zananvirani/Desktop/PrivInspect/backend/.venv/bin/python\n",
      "   Installation prefix: /Users/zananvirani/Desktop/PrivInspect/backend/.venv\n",
      "   Virtual environment: Yes\n",
      "   Working directory: /Users/zananvirani/Desktop/PrivInspect/notebooks\n",
      "‚úÖ All imports successful!\n",
      "\n",
      "üì¶ Package locations:\n",
      "   pandas: /Users/zananvirani/Desktop/PrivInspect/backend/.venv/lib/python3.13/site-packages/pandas/__init__.py\n",
      "   numpy: /Users/zananvirani/Desktop/PrivInspect/backend/.venv/lib/python3.13/site-packages/numpy/__init__.py\n",
      "   sklearn: /Users/zananvirani/Desktop/PrivInspect/backend/.venv/lib/python3.13/site-packages/sklearn/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Verify virtual environment and install dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Fix matplotlib backend issue before any imports\n",
    "os.environ['MPLBACKEND'] = 'Agg'\n",
    "\n",
    "def check_virtual_environment():\n",
    "    \"\"\"Check if we're running in a virtual environment\"\"\"\n",
    "    # Check for virtual environment indicators\n",
    "    venv_indicators = [\n",
    "        hasattr(sys, 'real_prefix'),  # virtualenv\n",
    "        (hasattr(sys, 'base_prefix') and sys.base_prefix != sys.prefix),  # venv\n",
    "        os.environ.get('VIRTUAL_ENV'),  # environment variable\n",
    "        os.environ.get('CONDA_DEFAULT_ENV')  # conda environment\n",
    "    ]\n",
    "    \n",
    "    is_venv = any(venv_indicators)\n",
    "    \n",
    "    if is_venv:\n",
    "        venv_path = os.environ.get('VIRTUAL_ENV', 'Unknown')\n",
    "        venv_name = os.path.basename(venv_path) if venv_path != 'Unknown' else 'conda environment'\n",
    "        print(f\"‚úÖ Virtual environment detected: {venv_name}\")\n",
    "        print(f\"   Environment path: {venv_path}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No virtual environment detected!\")\n",
    "        print(\"   Recommendation: Create and activate a virtual environment first:\")\n",
    "        print(\"   $ python3 -m venv .venv\")\n",
    "        print(\"   $ source .venv/bin/activate  # On Windows: .venv\\\\Scripts\\\\activate\")\n",
    "        print(\"   Then restart this notebook.\")\n",
    "        print(\"\\n   Continuing with system Python (not recommended for production)...\")\n",
    "        return False\n",
    "\n",
    "def install_package(package_name, import_name=None):\n",
    "    \"\"\"Install a package and verify it can be imported\"\"\"\n",
    "    if import_name is None:\n",
    "        import_name = package_name\n",
    "    \n",
    "    try:\n",
    "        __import__(import_name)\n",
    "        print(f\"‚úÖ {package_name} already installed\")\n",
    "        return True\n",
    "    except ImportError:\n",
    "        print(f\"‚è≥ Installing {package_name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
    "            print(f\"‚úÖ {package_name} installed successfully\")\n",
    "            return True\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"‚ùå Failed to install {package_name}: {e}\")\n",
    "            return False\n",
    "\n",
    "# Check environment first\n",
    "print(\"üîç Checking Python environment...\")\n",
    "venv_active = check_virtual_environment()\n",
    "\n",
    "# Core dependencies for ML and data processing\n",
    "packages = [\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"numpy\", \"numpy\"), \n",
    "    (\"scikit-learn\", \"sklearn\"),\n",
    "    (\"lightgbm\", \"lightgbm\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"seaborn\", \"seaborn\"),\n",
    "    (\"requests\", \"requests\"),\n",
    "    (\"joblib\", \"joblib\"),\n",
    "    (\"tqdm\", \"tqdm\")\n",
    "]\n",
    "\n",
    "print(f\"\\nüîß Installing required packages to: {sys.prefix}\")\n",
    "failed_packages = []\n",
    "\n",
    "for package_name, import_name in packages:\n",
    "    success = install_package(package_name, import_name)\n",
    "    if not success:\n",
    "        failed_packages.append(package_name)\n",
    "\n",
    "if failed_packages:\n",
    "    print(f\"‚ùå Failed to install: {failed_packages}\")\n",
    "    print(\"Please install these manually and rerun the notebook\")\n",
    "else:\n",
    "    print(\"‚úÖ All packages installed successfully!\")\n",
    "    \n",
    "# Display environment info\n",
    "print(f\"\\nüêç Python Environment Info:\")\n",
    "print(f\"   Python version: {sys.version}\")\n",
    "print(f\"   Python executable: {sys.executable}\")\n",
    "print(f\"   Installation prefix: {sys.prefix}\")\n",
    "print(f\"   Virtual environment: {'Yes' if venv_active else 'No (System Python)'}\")\n",
    "print(f\"   Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Try to import all packages to verify installation\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import sklearn\n",
    "    import lightgbm as lgb\n",
    "    import requests\n",
    "    import joblib\n",
    "    from tqdm import tqdm\n",
    "    print(\"‚úÖ All imports successful!\")\n",
    "    \n",
    "    # Show where packages are installed\n",
    "    print(f\"\\nüì¶ Package locations:\")\n",
    "    print(f\"   pandas: {pd.__file__}\")\n",
    "    print(f\"   numpy: {np.__file__}\")\n",
    "    print(f\"   sklearn: {sklearn.__file__}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"Please restart the notebook kernel and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724cc66",
   "metadata": {},
   "source": [
    "# Step 2: Project Setup and Data Download\n",
    "\n",
    "Creates the necessary project directory structure and defines the TrackerRadar data download functionality. This sets up everything needed to fetch and organize the DuckDuckGo TrackerRadar dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd234434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created/verified directory: ../data\n",
      "üìÅ Created/verified directory: ../models\n",
      "üìÅ Created/verified directory: ../scripts\n",
      "‚úÖ TrackerRadarParser class defined\n",
      "üöÄ Ready to download TrackerRadar data!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Setup project structure and create training module\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create necessary directories\n",
    "directories = ['../data', '../models', '../scripts']\n",
    "for dir_path in directories:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"üìÅ Created/verified directory: {dir_path}\")\n",
    "\n",
    "# Define the complete training module inline (for standalone operation)\n",
    "class TrackerRadarParser:\n",
    "    \"\"\"Parser for DuckDuckGo TrackerRadar data\"\"\"\n",
    "    \n",
    "    def __init__(self, base_url=\"https://github.com/duckduckgo/tracker-radar/archive/refs/heads/main.zip\"):\n",
    "        self.base_url = base_url\n",
    "        self.data_dir = Path(\"../data\")\n",
    "        \n",
    "    def download_tracker_radar(self) -> Dict:\n",
    "        \"\"\"Download and parse TrackerRadar data\"\"\"\n",
    "        import zipfile\n",
    "        import tempfile\n",
    "        \n",
    "        print(\"üì• Downloading DuckDuckGo TrackerRadar data...\")\n",
    "        \n",
    "        # Download the zip file\n",
    "        response = requests.get(self.base_url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Extract to temporary directory\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            zip_path = Path(temp_dir) / \"tracker-radar.zip\"\n",
    "            with open(zip_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            \n",
    "            # Extract zip\n",
    "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(temp_dir)\n",
    "            \n",
    "            # Find the extracted folder\n",
    "            extracted_folders = [d for d in Path(temp_dir).iterdir() if d.is_dir() and 'tracker-radar' in d.name]\n",
    "            if not extracted_folders:\n",
    "                raise ValueError(\"Could not find tracker-radar folder in downloaded zip\")\n",
    "            \n",
    "            tracker_radar_path = extracted_folders[0]\n",
    "            \n",
    "            # Copy to our data directory\n",
    "            import shutil\n",
    "            target_path = self.data_dir / \"tracker-radar\"\n",
    "            if target_path.exists():\n",
    "                shutil.rmtree(target_path)\n",
    "            shutil.copytree(tracker_radar_path, target_path)\n",
    "            \n",
    "            print(f\"‚úÖ TrackerRadar data downloaded to {target_path}\")\n",
    "            \n",
    "            # Return basic info about the data\n",
    "            domains_path = target_path / \"domains\"\n",
    "            if domains_path.exists():\n",
    "                # Count domain files across all countries\n",
    "                domain_count = 0\n",
    "                country_dirs = [d for d in domains_path.iterdir() if d.is_dir()]\n",
    "                for country_dir in country_dirs:\n",
    "                    json_files = list(country_dir.glob(\"*.json\"))\n",
    "                    domain_count += len(json_files)\n",
    "                \n",
    "                return {\n",
    "                    \"domains\": domain_count,\n",
    "                    \"countries\": len(country_dirs),\n",
    "                    \"data_path\": str(target_path)\n",
    "                }\n",
    "            else:\n",
    "                raise ValueError(\"Downloaded data does not contain expected domains directory\")\n",
    "\n",
    "print(\"‚úÖ TrackerRadarParser class defined\")\n",
    "print(\"üöÄ Ready to download TrackerRadar data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae63c74a",
   "metadata": {},
   "source": [
    "# Step 3: Execute Data Download\n",
    "\n",
    "Downloads the complete DuckDuckGo TrackerRadar dataset from GitHub, extracts it, and verifies the download. This provides the raw data for training the ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4da4d069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading DuckDuckGo TrackerRadar data...\n",
      "‚úÖ TrackerRadar data downloaded to ../data/tracker-radar\n",
      "‚úÖ TrackerRadar data downloaded to ../data/tracker-radar\n",
      "üìä TrackerRadar Data Summary:\n",
      "   Total domains: 51,841\n",
      "   Countries: 9\n",
      "   Data location: ../data/tracker-radar\n",
      "‚úÖ TrackerRadar data successfully downloaded and verified!\n",
      "üìä TrackerRadar Data Summary:\n",
      "   Total domains: 51,841\n",
      "   Countries: 9\n",
      "   Data location: ../data/tracker-radar\n",
      "‚úÖ TrackerRadar data successfully downloaded and verified!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Download TrackerRadar data\n",
    "parser = TrackerRadarParser()\n",
    "\n",
    "try:\n",
    "    # Download the data\n",
    "    tracker_info = parser.download_tracker_radar()\n",
    "    \n",
    "    print(\"üìä TrackerRadar Data Summary:\")\n",
    "    print(f\"   Total domains: {tracker_info['domains']:,}\")\n",
    "    print(f\"   Countries: {tracker_info['countries']}\")\n",
    "    print(f\"   Data location: {tracker_info['data_path']}\")\n",
    "    \n",
    "    # Verify the download\n",
    "    data_path = Path(tracker_info['data_path'])\n",
    "    if data_path.exists():\n",
    "        print(\"‚úÖ TrackerRadar data successfully downloaded and verified!\")\n",
    "    else:\n",
    "        print(\"‚ùå Data verification failed\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to download TrackerRadar data: {e}\")\n",
    "    print(\"Please check your internet connection and try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880500db",
   "metadata": {},
   "source": [
    "# Step 4: Define Advanced Feature Extraction\n",
    "\n",
    "Creates the domain feature extraction system with aggressive privacy detection improvements. This includes enhanced category-based tracking detection, legitimate domain protection, and reduced importance of resource counts for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12156d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DomainFeatureExtractor class defined with aggressive improvements\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Define domain feature extraction with aggressive improvements\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "class DomainFeatureExtractor:\n",
    "    \"\"\"Extract features from TrackerRadar domain data with aggressive privacy detection\"\"\"\n",
    "    \n",
    "    def __init__(self, data_path: str):\n",
    "        self.data_path = Path(data_path)\n",
    "        self.domains_path = self.data_path / \"domains\"\n",
    "        \n",
    "    def parse_domain_json(self, json_path: Path) -> Optional[Dict]:\n",
    "        \"\"\"Parse a single domain JSON file with aggressive category-based scoring\"\"\"\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            domain_name = json_path.stem\n",
    "            \n",
    "            # Get base fingerprinting score\n",
    "            base_fingerprinting = data.get('fingerprinting', 0)\n",
    "            \n",
    "            # AGGRESSIVE category-based tracking detection\n",
    "            categories = data.get('categories', [])\n",
    "            tracking_categories = [\n",
    "                'Ad Motivated Tracking', 'Advertising', 'Analytics', \n",
    "                'Audience Measurement', 'Third-Party Analytics Marketing',\n",
    "                'Cross-site Tracking', 'Fingerprinting'\n",
    "            ]\n",
    "            \n",
    "            # Count tracking categories with very aggressive weighting\n",
    "            tracking_category_count = sum(1 for cat in categories if cat in tracking_categories)\n",
    "            \n",
    "            # Additional aggressive categories\n",
    "            aggressive_categories = ['Social - Share', 'Embedded Content']\n",
    "            aggressive_count = sum(1 for cat in categories if cat in aggressive_categories)\n",
    "            \n",
    "            # VERY AGGRESSIVE scoring for multi-category tracking domains\n",
    "            if tracking_category_count >= 4:  # Domains with 4+ tracking categories\n",
    "                category_tracking_score = min(3, 2.5 + (tracking_category_count * 0.3) + (aggressive_count * 0.5))\n",
    "            elif tracking_category_count >= 2:  # Domains with 2-3 tracking categories  \n",
    "                category_tracking_score = min(3, 2.0 + (tracking_category_count * 0.4) + (aggressive_count * 0.4))\n",
    "            else:  # Single or no tracking categories\n",
    "                category_tracking_score = min(3, (tracking_category_count * 1.0) + (aggressive_count * 1.5))\n",
    "            \n",
    "            # Use maximum of base fingerprinting and enhanced category score\n",
    "            enhanced_fingerprinting = max(base_fingerprinting, category_tracking_score)\n",
    "            \n",
    "            # Reduce num_resources importance by 70% (aggressive)\n",
    "            raw_num_resources = len(data.get('resources', []))\n",
    "            scaled_num_resources = raw_num_resources * 0.3  # 70% reduction\n",
    "            \n",
    "            # Legitimate domain allowlisting\n",
    "            legitimate_domains = {\n",
    "                'wikipedia.org', 'archive.org', 'mozilla.org', 'github.com',\n",
    "                'stackoverflow.com', 'reddit.com', 'medium.com', 'twitter.com'\n",
    "            }\n",
    "            \n",
    "            # Reduce global_prevalence impact for legitimate sites\n",
    "            global_prevalence = data.get('prevalence', 0.0)\n",
    "            if domain_name in legitimate_domains:\n",
    "                global_prevalence = min(global_prevalence, 0.0005)  # Aggressive capping\n",
    "            \n",
    "            # Extract comprehensive features\n",
    "            resources = data.get('resources', [])\n",
    "            resource_types = {}\n",
    "            resource_fingerprinting_scores = []\n",
    "            \n",
    "            for resource in resources:\n",
    "                res_type = resource.get('type', 'unknown')\n",
    "                resource_types[res_type] = resource_types.get(res_type, 0) + 1\n",
    "                \n",
    "                res_fingerprinting = resource.get('fingerprinting', 0)\n",
    "                if res_fingerprinting > 0:\n",
    "                    resource_fingerprinting_scores.append(res_fingerprinting)\n",
    "            \n",
    "            features = {\n",
    "                'domain': domain_name,\n",
    "                'fingerprinting': enhanced_fingerprinting,\n",
    "                'cookies_prevalence': data.get('cookies', 0.0),\n",
    "                'global_prevalence': global_prevalence,\n",
    "                'num_sites': data.get('sites', 0),\n",
    "                'num_subdomains': len(data.get('subdomains', [])),\n",
    "                'num_cnames': len(data.get('cnames', [])),\n",
    "                'num_resources': scaled_num_resources,\n",
    "                'num_top_initiators': len(data.get('topInitiators', [])),\n",
    "                'owner_present': 1 if data.get('owner') else 0,\n",
    "                'resource_type_script_count': resource_types.get('script', 0),\n",
    "                'resource_type_xhr_count': resource_types.get('xmlhttprequest', 0),\n",
    "                'resource_type_image_count': resource_types.get('image', 0),\n",
    "                'resource_type_css_count': resource_types.get('stylesheet', 0),\n",
    "                'resource_type_font_count': resource_types.get('font', 0),\n",
    "                'resource_type_media_count': resource_types.get('media', 0),\n",
    "                'avg_resource_fingerprinting': np.mean(resource_fingerprinting_scores) if resource_fingerprinting_scores else 0.0,\n",
    "                'has_example_sites': 1 if data.get('exampleSites') else 0\n",
    "            }\n",
    "            \n",
    "            return features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {json_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_all_features(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Extract features from all domain files\"\"\"\n",
    "        all_features = {}\n",
    "        \n",
    "        if not self.domains_path.exists():\n",
    "            raise ValueError(f\"Domains directory not found: {self.domains_path}\")\n",
    "        \n",
    "        # Process all country directories\n",
    "        country_dirs = [d for d in self.domains_path.iterdir() if d.is_dir()]\n",
    "        total_processed = 0\n",
    "        \n",
    "        print(f\"üîç Processing domains from {len(country_dirs)} countries...\")\n",
    "        \n",
    "        for country_dir in country_dirs:\n",
    "            print(f\"üìç Processing {country_dir.name}...\")\n",
    "            json_files = list(country_dir.glob(\"*.json\"))\n",
    "            \n",
    "            for i, json_file in enumerate(json_files):\n",
    "                features = self.parse_domain_json(json_file)\n",
    "                if features:\n",
    "                    all_features[features['domain']] = features\n",
    "                    total_processed += 1\n",
    "                \n",
    "                # Progress update every 1000 files\n",
    "                if (i + 1) % 1000 == 0:\n",
    "                    print(f\"   Processed {i + 1:,}/{len(json_files):,} files\")\n",
    "        \n",
    "        print(f\"‚úÖ Extracted features for {total_processed:,} domains\")\n",
    "        return all_features\n",
    "\n",
    "print(\"‚úÖ DomainFeatureExtractor class defined with aggressive improvements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4fde08",
   "metadata": {},
   "source": [
    "# Step 5: Process All Domain Data\n",
    "\n",
    "Extracts features from all ~51,000+ domains in the TrackerRadar dataset. This processes each domain's JSON file to create the training dataset with comprehensive privacy-focused features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d4f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting feature extraction from all TrackerRadar domains...\n",
      "This may take 5-10 minutes for ~51,000 domains...\n",
      "üîç Processing domains from 9 countries...\n",
      "üìç Processing US...\n",
      "   Processed 1,000/23,553 files\n",
      "   Processed 1,000/23,553 files\n",
      "   Processed 2,000/23,553 files\n",
      "   Processed 2,000/23,553 files\n",
      "   Processed 3,000/23,553 files\n",
      "   Processed 3,000/23,553 files\n",
      "   Processed 4,000/23,553 files\n",
      "   Processed 4,000/23,553 files\n",
      "   Processed 5,000/23,553 files\n",
      "   Processed 5,000/23,553 files\n",
      "   Processed 6,000/23,553 files\n",
      "   Processed 6,000/23,553 files\n",
      "   Processed 7,000/23,553 files\n",
      "   Processed 7,000/23,553 files\n",
      "   Processed 8,000/23,553 files\n",
      "   Processed 8,000/23,553 files\n",
      "   Processed 9,000/23,553 files\n",
      "   Processed 9,000/23,553 files\n",
      "   Processed 10,000/23,553 files\n",
      "   Processed 10,000/23,553 files\n",
      "   Processed 11,000/23,553 files\n",
      "   Processed 11,000/23,553 files\n",
      "   Processed 12,000/23,553 files\n",
      "   Processed 12,000/23,553 files\n",
      "   Processed 13,000/23,553 files\n",
      "   Processed 13,000/23,553 files\n",
      "   Processed 14,000/23,553 files\n",
      "   Processed 14,000/23,553 files\n",
      "   Processed 15,000/23,553 files\n",
      "   Processed 15,000/23,553 files\n",
      "   Processed 16,000/23,553 files\n",
      "   Processed 16,000/23,553 files\n",
      "   Processed 17,000/23,553 files\n",
      "   Processed 17,000/23,553 files\n",
      "   Processed 18,000/23,553 files\n",
      "   Processed 18,000/23,553 files\n",
      "   Processed 19,000/23,553 files\n",
      "   Processed 19,000/23,553 files\n",
      "   Processed 20,000/23,553 files\n",
      "   Processed 20,000/23,553 files\n",
      "   Processed 21,000/23,553 files\n",
      "   Processed 21,000/23,553 files\n",
      "   Processed 22,000/23,553 files\n",
      "   Processed 22,000/23,553 files\n",
      "   Processed 23,000/23,553 files\n",
      "   Processed 23,000/23,553 files\n",
      "üìç Processing NO...\n",
      "üìç Processing NO...\n",
      "   Processed 1,000/3,567 files\n",
      "   Processed 1,000/3,567 files\n",
      "   Processed 2,000/3,567 files\n",
      "   Processed 2,000/3,567 files\n",
      "   Processed 3,000/3,567 files\n",
      "   Processed 3,000/3,567 files\n",
      "üìç Processing CH...\n",
      "üìç Processing CH...\n",
      "   Processed 1,000/3,414 files\n",
      "   Processed 1,000/3,414 files\n",
      "   Processed 2,000/3,414 files\n",
      "   Processed 2,000/3,414 files\n",
      "   Processed 3,000/3,414 files\n",
      "üìç Processing CA...\n",
      "   Processed 3,000/3,414 files\n",
      "üìç Processing CA...\n",
      "   Processed 1,000/4,164 files\n",
      "   Processed 1,000/4,164 files\n",
      "   Processed 2,000/4,164 files\n",
      "   Processed 2,000/4,164 files\n",
      "   Processed 3,000/4,164 files\n",
      "   Processed 3,000/4,164 files\n",
      "   Processed 4,000/4,164 files\n",
      "üìç Processing GB...\n",
      "   Processed 4,000/4,164 files\n",
      "üìç Processing GB...\n",
      "   Processed 1,000/3,622 files\n",
      "   Processed 1,000/3,622 files\n",
      "   Processed 2,000/3,622 files\n",
      "   Processed 2,000/3,622 files\n",
      "   Processed 3,000/3,622 files\n",
      "   Processed 3,000/3,622 files\n",
      "üìç Processing AU...\n",
      "üìç Processing AU...\n",
      "   Processed 1,000/4,022 files\n",
      "   Processed 1,000/4,022 files\n",
      "   Processed 2,000/4,022 files\n",
      "   Processed 2,000/4,022 files\n",
      "   Processed 3,000/4,022 files\n",
      "   Processed 3,000/4,022 files\n",
      "   Processed 4,000/4,022 files\n",
      "üìç Processing NL...\n",
      "   Processed 4,000/4,022 files\n",
      "üìç Processing NL...\n",
      "   Processed 1,000/3,327 files\n",
      "   Processed 1,000/3,327 files\n",
      "   Processed 2,000/3,327 files\n",
      "   Processed 2,000/3,327 files\n",
      "   Processed 3,000/3,327 files\n",
      "üìç Processing DE...\n",
      "   Processed 3,000/3,327 files\n",
      "üìç Processing DE...\n",
      "   Processed 1,000/3,010 files\n",
      "   Processed 1,000/3,010 files\n",
      "   Processed 2,000/3,010 files\n",
      "   Processed 2,000/3,010 files\n",
      "   Processed 3,000/3,010 files\n",
      "üìç Processing FR...\n",
      "   Processed 3,000/3,010 files\n",
      "üìç Processing FR...\n",
      "   Processed 1,000/3,162 files\n",
      "   Processed 1,000/3,162 files\n",
      "   Processed 2,000/3,162 files\n",
      "   Processed 2,000/3,162 files\n",
      "   Processed 3,000/3,162 files\n",
      "‚úÖ Extracted features for 51,841 domains\n",
      "\n",
      "üìä Feature Extraction Summary:\n",
      "   Total domains processed: 26,026\n",
      "\n",
      "üîç Sample domain features:\n",
      "\n",
      "d22p9r14gd7p23.cloudfront.net:\n",
      "   fingerprinting: 0\n",
      "   cookies_prevalence: 0\n",
      "   global_prevalence: 2.06e-05\n",
      "   num_sites: 3\n",
      "   num_subdomains: 0\n",
      "   num_cnames: 1\n",
      "   num_resources: 0.8999999999999999\n",
      "\n",
      "cdn-redge.media:\n",
      "   fingerprinting: 1\n",
      "   cookies_prevalence: 0\n",
      "   global_prevalence: 6.87e-06\n",
      "   num_sites: 1\n",
      "   num_subdomains: 2\n",
      "   num_cnames: 0\n",
      "   num_resources: 0.6\n",
      "\n",
      "bachmans-integrations.azurewebsites.net:\n",
      "   fingerprinting: 0\n",
      "   cookies_prevalence: 0\n",
      "   global_prevalence: 6.87e-06\n",
      "   num_sites: 1\n",
      "   num_subdomains: 0\n",
      "   num_cnames: 0\n",
      "   num_resources: 1.2\n",
      "   Processed 3,000/3,162 files\n",
      "‚úÖ Extracted features for 51,841 domains\n",
      "\n",
      "üìä Feature Extraction Summary:\n",
      "   Total domains processed: 26,026\n",
      "\n",
      "üîç Sample domain features:\n",
      "\n",
      "d22p9r14gd7p23.cloudfront.net:\n",
      "   fingerprinting: 0\n",
      "   cookies_prevalence: 0\n",
      "   global_prevalence: 2.06e-05\n",
      "   num_sites: 3\n",
      "   num_subdomains: 0\n",
      "   num_cnames: 1\n",
      "   num_resources: 0.8999999999999999\n",
      "\n",
      "cdn-redge.media:\n",
      "   fingerprinting: 1\n",
      "   cookies_prevalence: 0\n",
      "   global_prevalence: 6.87e-06\n",
      "   num_sites: 1\n",
      "   num_subdomains: 2\n",
      "   num_cnames: 0\n",
      "   num_resources: 0.6\n",
      "\n",
      "bachmans-integrations.azurewebsites.net:\n",
      "   fingerprinting: 0\n",
      "   cookies_prevalence: 0\n",
      "   global_prevalence: 6.87e-06\n",
      "   num_sites: 1\n",
      "   num_subdomains: 0\n",
      "   num_cnames: 0\n",
      "   num_resources: 1.2\n",
      "\n",
      "‚úÖ Features saved to: ../models/domain_features_notebook.json\n",
      "‚úÖ Feature extraction completed successfully!\n",
      "\n",
      "‚úÖ Features saved to: ../models/domain_features_notebook.json\n",
      "‚úÖ Feature extraction completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Extract features from all domains\n",
    "extractor = DomainFeatureExtractor(tracker_info['data_path'])\n",
    "\n",
    "print(\"üöÄ Starting feature extraction from all TrackerRadar domains...\")\n",
    "print(\"This may take 5-10 minutes for ~51,000 domains...\")\n",
    "\n",
    "# Extract features\n",
    "all_domain_features = extractor.extract_all_features()\n",
    "\n",
    "print(f\"\\nüìä Feature Extraction Summary:\")\n",
    "print(f\"   Total domains processed: {len(all_domain_features):,}\")\n",
    "\n",
    "# Show sample features from a few domains\n",
    "sample_domains = list(all_domain_features.keys())[:3]\n",
    "print(f\"\\nüîç Sample domain features:\")\n",
    "for domain in sample_domains:\n",
    "    features = all_domain_features[domain]\n",
    "    print(f\"\\n{domain}:\")\n",
    "    for key, value in list(features.items())[:8]:  # Show first 8 features\n",
    "        if key != 'domain':\n",
    "            print(f\"   {key}: {value}\")\n",
    "\n",
    "# Save features to file for later use\n",
    "features_file = Path(\"../models/domain_features.json\")\n",
    "with open(features_file, 'w') as f:\n",
    "    json.dump(all_domain_features, f)\n",
    "\n",
    "print(f\"\\n‚úÖ Features saved to: {features_file}\")\n",
    "print(\"‚úÖ Feature extraction completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94089b35",
   "metadata": {},
   "source": [
    "# Step 6: Train Machine Learning Model\n",
    "\n",
    "Creates training targets based on domain characteristics and trains a LightGBM model to predict domain tracking intensity. Includes model evaluation, feature importance analysis, and saves the trained model for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0bf34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Creating training targets...\n",
      "‚úÖ Created targets for 26,026 domains\n",
      "üèãÔ∏è Training LightGBM model...\n",
      "Training data shape: (26026, 17)\n",
      "Target range: 0.000 to 1.000\n",
      "\\nüìä Model Performance:\n",
      "   MAE: 0.0324\n",
      "   RMSE: 0.0415\n",
      "   Spearman correlation: 0.9005\n",
      "\\nüîù Top 10 Feature Importances:\n",
      "    1. global_prevalence: 656\n",
      "    2. num_resources: 592\n",
      "    3. fingerprinting: 237\n",
      "    4. cookies_prevalence: 210\n",
      "    5. num_sites: 204\n",
      "    6. avg_resource_fingerprinting: 191\n",
      "    7. num_subdomains: 178\n",
      "    8. num_top_initiators: 165\n",
      "    9. num_cnames: 77\n",
      "   10. owner_present: 67\n",
      "‚úÖ Model saved to: ../models/domain_risk_model_notebook.pkl\n",
      "‚úÖ Model training completed successfully!\n",
      "\\nüìä Model Performance:\n",
      "   MAE: 0.0324\n",
      "   RMSE: 0.0415\n",
      "   Spearman correlation: 0.9005\n",
      "\\nüîù Top 10 Feature Importances:\n",
      "    1. global_prevalence: 656\n",
      "    2. num_resources: 592\n",
      "    3. fingerprinting: 237\n",
      "    4. cookies_prevalence: 210\n",
      "    5. num_sites: 204\n",
      "    6. avg_resource_fingerprinting: 191\n",
      "    7. num_subdomains: 178\n",
      "    8. num_top_initiators: 165\n",
      "    9. num_cnames: 77\n",
      "   10. owner_present: 67\n",
      "‚úÖ Model saved to: ../models/domain_risk_model_notebook.pkl\n",
      "‚úÖ Model training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Create training targets and train ML model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Train the domain risk model with LightGBM\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def create_training_targets(self, features_dict: Dict) -> Dict[str, float]:\n",
    "        \"\"\"Create training targets based on domain characteristics\"\"\"\n",
    "        targets = {}\n",
    "        \n",
    "        print(\"üéØ Creating training targets...\")\n",
    "        \n",
    "        for domain, features in features_dict.items():\n",
    "            # Base tracking intensity from fingerprinting\n",
    "            base_intensity = min(features['fingerprinting'] / 3.0, 1.0)\n",
    "            \n",
    "            # Boost for high cookie prevalence\n",
    "            cookie_boost = min(features['cookies_prevalence'] * 0.3, 0.3)\n",
    "            \n",
    "            # Boost for high prevalence (indicates tracking)\n",
    "            prevalence_boost = min(features['global_prevalence'] * 100, 0.2)\n",
    "            \n",
    "            # Resource-based tracking signals\n",
    "            resource_boost = 0\n",
    "            if features['num_resources'] > 10:  # Already scaled down\n",
    "                resource_boost = 0.1\n",
    "            if features['avg_resource_fingerprinting'] > 1:\n",
    "                resource_boost += 0.1\n",
    "                \n",
    "            # Combine all factors\n",
    "            tracking_intensity = min(base_intensity + cookie_boost + prevalence_boost + resource_boost, 1.0)\n",
    "            \n",
    "            # Apply some noise for regularization\n",
    "            noise = np.random.normal(0, 0.05)\n",
    "            tracking_intensity = max(0.0, min(1.0, tracking_intensity + noise))\n",
    "            \n",
    "            targets[domain] = tracking_intensity\n",
    "        \n",
    "        print(f\"‚úÖ Created targets for {len(targets):,} domains\")\n",
    "        return targets\n",
    "    \n",
    "    def train_model(self, features_dict: Dict, targets_dict: Dict):\n",
    "        \"\"\"Train the LightGBM model\"\"\"\n",
    "        print(\"üèãÔ∏è Training LightGBM model...\")\n",
    "        \n",
    "        # Prepare data\n",
    "        domains = list(features_dict.keys())\n",
    "        \n",
    "        # Create feature matrix\n",
    "        feature_names = [k for k in features_dict[domains[0]].keys() if k != 'domain']\n",
    "        self.feature_names = feature_names\n",
    "        \n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for domain in domains:\n",
    "            if domain in targets_dict:\n",
    "                features = features_dict[domain]\n",
    "                feature_row = [features[name] for name in feature_names]\n",
    "                X.append(feature_row)\n",
    "                y.append(targets_dict[domain])\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        print(f\"Training data shape: {X.shape}\")\n",
    "        print(f\"Target range: {y.min():.3f} to {y.max():.3f}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Scale features\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        # Train LightGBM model\n",
    "        try:\n",
    "            import lightgbm as lgb\n",
    "            \n",
    "            self.model = lgb.LGBMRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.1,\n",
    "                random_state=42,\n",
    "                verbose=-1\n",
    "            )\n",
    "            \n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred = self.model.predict(X_test_scaled)\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            correlation, _ = spearmanr(y_test, y_pred)\n",
    "            \n",
    "            print(f\"\\\\nüìä Model Performance:\")\n",
    "            print(f\"   MAE: {mae:.4f}\")\n",
    "            print(f\"   RMSE: {rmse:.4f}\")\n",
    "            print(f\"   Spearman correlation: {correlation:.4f}\")\n",
    "            \n",
    "            # Feature importance\n",
    "            importances = self.model.feature_importances_\n",
    "            feature_importance = list(zip(feature_names, importances))\n",
    "            feature_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            print(f\"\\\\nüîù Top 10 Feature Importances:\")\n",
    "            for i, (feature, importance) in enumerate(feature_importance[:10]):\n",
    "                print(f\"   {i+1:2d}. {feature}: {importance:.0f}\")\n",
    "                \n",
    "            return True\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ùå LightGBM not available, using RandomForest as fallback...\")\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            \n",
    "            self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            y_pred = self.model.predict(X_test_scaled)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            \n",
    "            print(f\"\\\\nüìä Model Performance (RandomForest):\")\n",
    "            print(f\"   MAE: {mae:.4f}\")\n",
    "            print(f\"   RMSE: {rmse:.4f}\")\n",
    "            \n",
    "            return True\n",
    "    \n",
    "    def save_model(self, save_dir: str):\n",
    "        \"\"\"Save the trained model\"\"\"\n",
    "        save_path = Path(save_dir)\n",
    "        save_path.mkdir(exist_ok=True)\n",
    "        \n",
    "        model_artifacts = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names,\n",
    "            'model_type': 'lightgbm' if hasattr(self.model, 'feature_importances_') else 'random_forest'\n",
    "        }\n",
    "        \n",
    "        model_file = save_path / \"domain_risk_model.pkl\"\n",
    "        joblib.dump(model_artifacts, model_file)\n",
    "        \n",
    "        print(f\"‚úÖ Model saved to: {model_file}\")\n",
    "        return str(model_file)\n",
    "\n",
    "# Initialize and train the model\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Create targets\n",
    "targets = trainer.create_training_targets(all_domain_features)\n",
    "\n",
    "# Train model\n",
    "success = trainer.train_model(all_domain_features, targets)\n",
    "\n",
    "if success:\n",
    "    # Save model\n",
    "    model_path = trainer.save_model(\"../models\")\n",
    "    print(\"‚úÖ Model training completed successfully!\")\n",
    "else:\n",
    "    print(\"‚ùå Model training failed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
